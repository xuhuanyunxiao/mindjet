{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "* http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
    "> * 1 Standardization, or mean removal and variance scaling\n",
    "> * 2 Normalization\n",
    "> * 3 Binarization\n",
    "> * 4 Encoding categorical features\n",
    "> * 5 Imputation of missing values\n",
    "> * 6 Generating polynomial features\n",
    "> * 7 Custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GaussianProcess',\n",
       " 'GaussianProcessClassifier',\n",
       " 'GaussianProcessRegressor',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'correlation_models',\n",
       " 'gaussian_process',\n",
       " 'gpc',\n",
       " 'gpr',\n",
       " 'kernels',\n",
       " 'regression_models']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.gaussian_process \n",
    "dir(sklearn.gaussian_process )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7174c6d94521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "dir(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Standardization, or mean removal and variance scaling：标准化，或去均值和方差尺度\n",
    "* In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "* 标准化是将每个特征的数据统一到相同的尺度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 用同一套尺度标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "\n",
      "[ 0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "print(X_scaled)    \n",
    "print()\n",
    "print(X_scaled.mean(axis=0))\n",
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 用训练数据的均值、平均差来标准化测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "\n",
      "[ 1.          0.          0.33333333]\n",
      "\n",
      "[ 0.81649658  0.81649658  1.24721913]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X) # to disable either centering or scaling by either passing with_mean=False or with_std=False\n",
    "\n",
    "print(scaler)\n",
    "print()\n",
    "print(scaler.mean_) \n",
    "print()\n",
    "print(scaler.scale_)                                       \n",
    "scaler.transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Scaling features to a range：标准化到某个范围，如[0 1]\n",
    "> * The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         0.          1.        ]\n",
      " [ 1.          0.5         0.33333333]\n",
      " [ 0.          1.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MinMaxScaler\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()  # 默认：[0 1]，可更改 feature_range=(min, max)\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "print(X_train_minmax)\n",
    "\n",
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5 -1.   1. ]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.  -0.5]]\n",
      "\n",
      "[[-1.5 -1.   2. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  2.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MaxAbsScaler: It is meant for data that is already centered at zero or sparse data.\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "print(X_train_maxabs)                # doctest +NORMALIZE_WHITESPACE^\n",
    "print()\n",
    "\n",
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "print(X_test_maxabs)                 \n",
    "max_abs_scaler.scale_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Scaling sparse data: 稀疏数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go about this. \n",
    "* However, scale and StandardScaler can accept scipy.sparse matrices as input, as long as with_mean=False is explicitly passed to the constructor\n",
    "* \n",
    "* Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see scipy.sparse.csr_matrix and scipy.sparse.csc_matrix). \n",
    "* Any other sparse input will be converted to the Compressed Sparse Rows representation. \n",
    "* To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Scaling data with outliers：标准化带有离群值的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* use robust_scale and RobustScaler as drop-in replacements instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Scaling vs Whitening\n",
    "> * It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.\n",
    "> * To address this issue you can use sklearn.decomposition.PCA or sklearn.decomposition.RandomizedPCA with whiten=True to further remove the linear correlation across features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.5 Centering kernel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Normalization：归一化\n",
    "* Normalization is the process of scaling individual samples to have unit norm \n",
    "* 归一化是将每个样本的数据统一到相同尺度\n",
    "* This process can be useful if you plan to use a quadratic form（二次方形式） such as the dot-product or any other kernel to quantify the similarity of any pair of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2') # l1\n",
    "X_normalized     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer(copy=True, norm='l2')\n",
      "\n",
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678,  0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n",
    "print(normalizer)\n",
    "print()\n",
    "print(normalizer.transform(X))\n",
    "normalizer.transform([[-1.,  1., 0.]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sparse input\n",
    "> * normalize and Normalizer accept both dense array-like and sparse matrices from scipy.sparse as input.\n",
    "> * For sparse input the data is converted to the Compressed Sparse Rows representation (see scipy.sparse.csr_matrix) before being fed to efficient Cython routines. \n",
    "> * To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Binarization：二值化\n",
    "* Feature binarization is the process of thresholding numerical features to get boolean values. （布尔值）\n",
    "* This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. \n",
    "* For instance, this is the case for the sklearn.neural_network.BernoulliRBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer(copy=True, threshold=0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n",
    "print(binarizer)\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer = preprocessing.Binarizer(threshold=1.1) # 调整阈限\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Encoding categorical features：编码类别特征\n",
    "* One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K or one-hot encoding, which is implemented in OneHotEncoder. \n",
    "* This estimator transforms each categorical feature with m possible values into m binary features, with only one active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotEncoder(categorical_features='all', dtype=<class 'float'>,\n",
      "       handle_unknown='error', n_values='auto', sparse=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "print(enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]))  \n",
    "enc.transform([[0, 1, 3]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default, how many values each feature can take is inferred automatically from the dataset. \n",
    "* It is possible to specify this explicitly using the parameter n_values. \n",
    "> * There are two genders, three possible continents and four web browsers in our dataset. \n",
    "> * Then we fit the estimator, and transform a data point. \n",
    "> * In the result, the first two numbers encode the gender, the next set of three numbers the continent and the last four the web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotEncoder(categorical_features='all', dtype=<class 'float'>,\n",
      "       handle_unknown='error', n_values=[2, 3, 4], sparse=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(n_values=[2, 3, 4])\n",
    "# Note that there are missing categorical values for the 2nd and 3rd features\n",
    "print(enc.fit([[1, 2, 3], [0, 2, 0]])  )\n",
    "enc.transform([[1, 0, 0]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Imputation of missing values：处理缺失值\n",
    "* A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. \n",
    "* However, this comes at the price of losing data which may be valuable (even though incomplete). \n",
    "* A better strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n",
    "> * The Imputer class provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located. \n",
    "> * This class also allows for different missing values encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n",
      "\n",
      "[[nan, 2], [6, nan], [7, 6]]\n",
      "\n",
      "[[ 4.          2.        ]\n",
      " [ 6.          3.66666667]\n",
      " [ 7.          6.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "print(imp.fit([[1, 2], [np.nan, 3], [7, 6]]))\n",
    "print()\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(X)\n",
    "print()\n",
    "print(imp.transform(X))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)\n",
      "\n",
      "  (1, 0)\t6\n",
      "  (2, 0)\t7\n",
      "  (0, 1)\t2\n",
      "  (2, 1)\t6\n",
      "\n",
      "[[ 4.          2.        ]\n",
      " [ 6.          3.66666675]\n",
      " [ 7.          6.        ]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    "imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    "print(imp.fit(X))\n",
    "print()\n",
    "X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])  # 稀疏格式数据，missing values are encoded by 0 and are thus implicitly stored in the matrix\n",
    "print(X_test)\n",
    "print()\n",
    "print(imp.transform(X_test))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Generating polynomial features：构造多项式特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   0.,   0.,   1.],\n",
       "       [  1.,   2.,   3.,   4.,   6.,   9.],\n",
       "       [  1.,   4.,   5.,  16.,  20.,  25.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "print(X)\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The features of X have been transformed from (X_1, X_2) to (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   0.],\n",
       "       [  1.,   2.,   3.,   6.],\n",
       "       [  1.,   4.,   5.,  20.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(6).reshape(3, 2)\n",
    "print(X)                                           \n",
    "poly = PolynomialFeatures(degree=3, interaction_only=True) # In some cases, only interaction terms among features are required\n",
    "poly.fit_transform(X)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "* The features of X have been transformed from (X_1, X_2) to (1, X_1, X_2, X_1X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Custom transformers：自定义传递函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.69314718],\n",
       "       [ 1.09861229,  1.38629436]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p) # log transformation\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "print(X)\n",
    "transformer.transform(X)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
